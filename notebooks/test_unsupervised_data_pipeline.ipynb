{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Unsupervised Data Pipeline with Real Data\n",
        "\n",
        "This notebook tests the complete pipeline using **real data downloads**:\n",
        "- **Wikipedia**: Downloaded via Wikipedia API\n",
        "- **CC-News**: Downloaded via Hugging Face datasets\n",
        "- **BookCorpus**: Downloaded via Hugging Face datasets\n",
        "\n",
        "**Requirements**: `pip install datasets`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup complete!\n",
            "   Project root: /Users/stepan/Documents/RLML/SpellChecker\n",
            "   Temp directory: /var/folders/fx/th2v9glj5tz0jj1y5s8_5fjh0000gn/T/tmpabhghrte\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import tempfile\n",
        "import shutil\n",
        "import urllib.request\n",
        "\n",
        "# Add src to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Import our parsers\n",
        "from spellchecker.data.parsers.unsupervised_parser import (\n",
        "    WikipediaParser,\n",
        "    CCNewsParser,\n",
        "    BookCorpusParser,\n",
        "    UniversalTextCleaner,\n",
        ")\n",
        "\n",
        "# Create cleaner\n",
        "cleaner = UniversalTextCleaner(min_length=10, max_length=500)\n",
        "\n",
        "# Create temp directory\n",
        "temp_dir = Path(tempfile.mkdtemp())\n",
        "\n",
        "print(f\"Setup complete!\")\n",
        "print(f\"   Project root: {project_root}\")\n",
        "print(f\"   Temp directory: {temp_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading real Wikipedia articles...\n",
            "\n",
            "  ‚úì Downloaded: Python (programming language) (152 chars)\n",
            "  ‚úì Downloaded: Machine learning (462 chars)\n",
            "  ‚úì Downloaded: Natural language processing (331 chars)\n",
            "  ‚úì Downloaded: Artificial intelligence (463 chars)\n",
            "  ‚úì Downloaded: Data science (270 chars)\n",
            "\n",
            "‚úÖ Saved 5 Wikipedia articles\n",
            "   Total size: 1934 bytes\n"
          ]
        }
      ],
      "source": [
        "print(\"Downloading real Wikipedia articles...\\n\")\n",
        "\n",
        "# Create Wikipedia directory structure\n",
        "wiki_dir = temp_dir / \"wikipedia\" / \"extracted\" / \"AA\"\n",
        "wiki_dir.mkdir(parents=True, exist_ok=True)\n",
        "wiki_file = wiki_dir / \"wiki_00\"\n",
        "\n",
        "# Articles to download\n",
        "articles = [\n",
        "    \"Python_(programming_language)\",\n",
        "    \"Machine_learning\",\n",
        "    \"Natural_language_processing\",\n",
        "    \"Artificial_intelligence\",\n",
        "    \"Data_science\"\n",
        "]\n",
        "\n",
        "wiki_texts = []\n",
        "\n",
        "for article_title in articles:\n",
        "    try:\n",
        "        url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{article_title}\"\n",
        "        \n",
        "        # Create request with proper headers (Wikipedia requires User-Agent)\n",
        "        req = urllib.request.Request(\n",
        "            url,\n",
        "            headers={\n",
        "                'User-Agent': 'SpellCheckerBot/1.0 (Educational Project)',\n",
        "                'Accept': 'application/json'\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        with urllib.request.urlopen(req, timeout=10) as response:\n",
        "            data = json.loads(response.read())\n",
        "            \n",
        "            title = data.get(\"title\", article_title.replace(\"_\", \" \"))\n",
        "            extract = data.get(\"extract\", \"\")\n",
        "            \n",
        "            # Format as WikiExtractor-like output\n",
        "            wiki_text = f'<doc id=\"{len(wiki_texts)+1}\" title=\"{title}\">\\n{extract}\\n</doc>'\n",
        "            wiki_texts.append(wiki_text)\n",
        "            \n",
        "            print(f\"  ‚úì Downloaded: {title} ({len(extract)} chars)\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Failed to download {article_title}: {e}\")\n",
        "\n",
        "# If no articles downloaded, use sample data\n",
        "if len(wiki_texts) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  No articles downloaded from Wikipedia API\")\n",
        "    print(\"   Using sample data instead...\\n\")\n",
        "    \n",
        "    wiki_texts = [\n",
        "        \"\"\"<doc id=\"1\" title=\"Python\">\n",
        "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented and functional programming.\n",
        "</doc>\"\"\",\n",
        "        \"\"\"<doc id=\"2\" title=\"Machine Learning\">\n",
        "Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\n",
        "</doc>\"\"\",\n",
        "        \"\"\"<doc id=\"3\" title=\"Natural Language Processing\">\n",
        "Natural language processing is an interdisciplinary subfield of computer science and artificial intelligence. It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics.\n",
        "</doc>\"\"\",\n",
        "        \"\"\"<doc id=\"4\" title=\"Artificial Intelligence\">\n",
        "Artificial intelligence is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\n",
        "</doc>\"\"\",\n",
        "        \"\"\"<doc id=\"5\" title=\"Data Science\">\n",
        "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy, structured, and unstructured data.\n",
        "</doc>\"\"\"\n",
        "    ]\n",
        "    \n",
        "    for i, text in enumerate(wiki_texts, 1):\n",
        "        # Extract title for display\n",
        "        title_start = text.find('title=\"') + 7\n",
        "        title_end = text.find('\"', title_start)\n",
        "        title = text[title_start:title_end]\n",
        "        print(f\"  ‚úì Using sample: {title}\")\n",
        "\n",
        "# Write to file\n",
        "with open(wiki_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(wiki_texts))\n",
        "\n",
        "print(f\"\\nSaved {len(wiki_texts)} Wikipedia articles\")\n",
        "print(f\"   Total size: {wiki_file.stat().st_size} bytes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Processing Wikipedia articles with WikipediaParser...\n",
            "\n",
            "Processing /var/folders/fx/th2v9glj5tz0jj1y5s8_5fjh0000gn/T/tmpu4z853lu/wikipedia/extracted/AA/wiki_00...\n",
            "Saved 5 passages to /var/folders/fx/th2v9glj5tz0jj1y5s8_5fjh0000gn/T/tmpu4z853lu/wikipedia_processed.txt\n",
            "\n",
            "‚úÖ Processed 5 passages from Wikipedia\n",
            "\n",
            "üìÑ First 3 processed passages:\n",
            "  1. Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n",
            "  2. Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  3. Natural language processing (NLP) is the processing of natural language information by a computer. T...\n"
          ]
        }
      ],
      "source": [
        "# Test WikipediaParser on real data\n",
        "print(\"\\nProcessing Wikipedia articles with WikipediaParser...\\n\")\n",
        "\n",
        "wiki_parser = WikipediaParser(cleaner)\n",
        "output_file = temp_dir / \"wikipedia_processed.txt\"\n",
        "\n",
        "count = wiki_parser.save_to_file(\n",
        "    input_path=temp_dir / \"wikipedia\" / \"extracted\",\n",
        "    output_file=output_file\n",
        ")\n",
        "\n",
        "print(f\"\\nProcessed {count} passages from Wikipedia\")\n",
        "print(f\"\\nFirst 3 processed passages:\")\n",
        "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        if i > 3:\n",
        "            break\n",
        "        preview = line.strip()[:100] + \"...\" if len(line.strip()) > 100 else line.strip()\n",
        "        print(f\"  {i}. {preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading real CC-News articles...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/stepan/Documents/RLML/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Downloaded article 1: Daughter Duo is Dancing in The Same Company\n",
            "  ‚úì Downloaded article 2: New York City Ballet Announces Interim Leadership Team\n",
            "  ‚úì Downloaded article 3: Watch Pennsylvania Ballet & Boston Ballet Face Off for the S...\n",
            "  ‚úì Downloaded article 4: dance shoes\n",
            "  ‚úì Downloaded article 5: Rebecca Krohn on Her Retirement from New York City Ballet\n",
            "  ‚úì Downloaded article 6: Roy Kaiser to Become Nevada Ballet Theatre's New Artistic Di...\n",
            "  ‚úì Downloaded article 7: What It's Like Inside NYCB After Peter Martins\n",
            "  ‚úì Downloaded article 8: Nutcracker Secrets and Surprises\n",
            "  ‚úì Downloaded article 9: Inside the Beijing Dance Academy\n",
            "  ‚úì Downloaded article 10: dance shoes\n",
            "  ‚úì Downloaded article 11: Isabella Boylston and James Whiteside Get Hilariously Candid\n",
            "  ‚úì Downloaded article 12: Ballet Performances This Week\n",
            "  ‚úì Downloaded article 13: Guillaume C√¥t√© on NBoC's \"Frame by Frame\"\n",
            "  ‚úì Downloaded article 14: Broadway's \"Carousel\" Stars Some Familiar Ballet Faces\n",
            "  ‚úì Downloaded article 15: The Joffrey Presents Ekman's \"Midsummer Night's Dream\"\n",
            "  ‚úì Downloaded article 16: Wonderfully Simple Graphic Design Software\n",
            "  ‚úì Downloaded article 17: Bay Community News\n",
            "  ‚úì Downloaded article 18: 25 Year Old Molests Child Under 12\n",
            "  ‚úì Downloaded article 19: How to install the Trac project management tool on Ubuntu 16...\n",
            "  ‚úì Downloaded article 20: New Amazon class certifies cloud pros in securing data on AW...\n",
            "\n",
            "‚úÖ Downloaded 20 real CC-News articles\n",
            "   File size: 51705 bytes\n"
          ]
        }
      ],
      "source": [
        "print(\"üì• Downloading real CC-News articles...\\n\")\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    \n",
        "    # Download 20 articles from CC-News\n",
        "    dataset = load_dataset(\"cc_news\", split=\"train\", streaming=True)\n",
        "    \n",
        "    ccnews_file = temp_dir / \"ccnews.jsonl\"\n",
        "    count = 0\n",
        "    \n",
        "    with open(ccnews_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, example in enumerate(dataset):\n",
        "            if i >= 20:  # Download 20 articles\n",
        "                break\n",
        "            \n",
        "            title = example.get(\"title\", \"\")\n",
        "            text = example.get(\"text\", \"\")\n",
        "            \n",
        "            json.dump({\"title\": title, \"text\": text}, f)\n",
        "            f.write(\"\\n\")\n",
        "            count += 1\n",
        "            \n",
        "            title_preview = title[:60] + \"...\" if len(title) > 60 else title\n",
        "            print(f\"  ‚úì Downloaded article {count}: {title_preview}\")\n",
        "    \n",
        "    print(f\"\\nDownloaded {count} real CC-News articles\")\n",
        "    print(f\"   File size: {ccnews_file.stat().st_size} bytes\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"'datasets' library not available\")\n",
        "    print(\"   Install with: pip install datasets\")\n",
        "    ccnews_file = None\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading CC-News: {e}\")\n",
        "    ccnews_file = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Processing CC-News articles with CCNewsParser...\n",
            "\n",
            "Saved 3 articles to /var/folders/fx/th2v9glj5tz0jj1y5s8_5fjh0000gn/T/tmpu4z853lu/ccnews_processed.txt\n",
            "\n",
            "‚úÖ Processed 3 articles from CC-News\n",
            "\n",
            "üìÑ First 3 processed articles:\n",
            "  1. dance shoes. Looking for your next audition shoe? Shot at and in collaboration with Broadway Dance C...\n",
            "  2. dance shoes. Looking for your next audition shoe? Shot at and in collaboration with Broadway Dance C...\n",
            "  3. Wonderfully Simple Graphic Design Software. DesignWizard Sponsors rebelCon 2017 DesignWizard has joi...\n"
          ]
        }
      ],
      "source": [
        "# Test CCNewsParser on real data\n",
        "if ccnews_file:\n",
        "    print(\"\\nProcessing CC-News articles with CCNewsParser...\\n\")\n",
        "    \n",
        "    ccnews_parser = CCNewsParser(cleaner)\n",
        "    output_file = temp_dir / \"ccnews_processed.txt\"\n",
        "    \n",
        "    count = ccnews_parser.save_to_file(input_file=ccnews_file, output_file=output_file)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processed {count} articles from CC-News\")\n",
        "    print(f\"\\nüìÑ First 3 processed articles:\")\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            if i > 3:\n",
        "                break\n",
        "            preview = line.strip()[:100] + \"...\" if len(line.strip()) > 100 else line.strip()\n",
        "            print(f\"  {i}. {preview}\")\n",
        "else:\n",
        "    print(\"Skipping CC-News processing (download failed)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Real BookCorpus Data\n",
        "\n",
        "Download real book sentences from the BookCorpus dataset via Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading real BookCorpus data...\n",
            "\n",
            "  Loading BookCorpus dataset (streaming mode)...\n",
            "  Downloading sentences...\n",
            "\n",
            "  ‚úì Sentence 1: 1 + 2\n",
            "\n",
            "This Is Only The Beginning\n",
            "\n",
            "Kristie Lynn Higgins\n",
            "\n",
            "Tex...\n",
            "  ‚úì Sentence 2: ## 1 God ‚Äì Poems on God , Creator ‚Äì volume 1\n",
            "\n",
            "## By\n",
            "\n",
            "Nikhil ...\n",
            "  ‚úì Sentence 3: ## 1 God ‚Äì Poems on God , Creator ‚Äì volume 2\n",
            "\n",
            "## By\n",
            "\n",
            "Nikhil ...\n",
            "  ‚úì Sentence 4: ## 1 God ‚Äì Poems on God , Creator ‚Äì volume 3\n",
            "\n",
            "## By\n",
            "\n",
            "Nikhil ...\n",
            "  ‚úì Sentence 5: ## 1 God ‚Äì Poems on God , Creator ‚Äì volume 4\n",
            "\n",
            "## By\n",
            "\n",
            "Nikhil ...\n",
            "  ‚úì Sentence 10: ### 10 of the Best Stories from Kenji Miyazawa & Nankichi Ni...\n",
            "  ‚úì Sentence 20: * * *\n",
            "\n",
            "## One Thousand Yards\n",
            "\n",
            "A John Milton Novel\n",
            "\n",
            "Mark Daws...\n",
            "  ‚úì Sentence 30: # 10X Culture\n",
            "\n",
            "### The 4-hour meeting week and 25 other secr...\n",
            "  ‚úì Sentence 40: 13 TALES TO GIVE YOU NIGHT TERRORS\n",
            "\n",
            "A Night Terrors Novel\n",
            "\n",
            "E...\n",
            "  ‚úì Sentence 50: # 185 TIPS ON WORLD BUILDING\n",
            "\n",
            "# by Randy Ellefson\n",
            "\n",
            "Copyright...\n",
            "\n",
            "‚úÖ Downloaded 50 real BookCorpus sentences\n",
            "   File size: 17924923 bytes\n"
          ]
        }
      ],
      "source": [
        "print(\"Downloading real BookCorpus data...\\n\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the BookCorpus dataset using streaming mode\n",
        "print(\"  Loading BookCorpus dataset (streaming mode)...\")\n",
        "dataset = load_dataset(\"lucadiliello/bookcorpusopen\", split=\"train\", streaming=True)\n",
        "\n",
        "book_file = temp_dir / \"bookcorpus.txt\"\n",
        "\n",
        "# Extract first 50 sentences\n",
        "count = 0\n",
        "print(\"  Downloading sentences...\\n\")\n",
        "\n",
        "with open(book_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= 50:  # Take 50 sentences\n",
        "            break\n",
        "        \n",
        "        text = example[\"text\"].strip()\n",
        "        if text:\n",
        "            f.write(text + \"\\n\")\n",
        "            count += 1\n",
        "            \n",
        "            text_preview = text[:60] + \"...\" if len(text) > 60 else text\n",
        "            if count <= 5 or count % 10 == 0:  # Show first 5 and every 10th\n",
        "                print(f\"  ‚úì Sentence {count}: {text_preview}\")\n",
        "\n",
        "print(f\"\\nDownloaded {count} real BookCorpus sentences\")\n",
        "print(f\"   File size: {book_file.stat().st_size} bytes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Processing BookCorpus sentences with BookCorpusParser...\n",
            "\n",
            "Saved 106703 passages to /var/folders/fx/th2v9glj5tz0jj1y5s8_5fjh0000gn/T/tmpabhghrte/bookcorpus_processed.txt\n",
            "\n",
            "‚úÖ Processed 106703 passages from BookCorpus\n",
            "\n",
            "üìÑ First 3 processed passages:\n",
            "  1. This Is Only The Beginning\n",
            "  2. Kristie Lynn Higgins\n",
            "  3. Text Copyright ¬© 2018\n"
          ]
        }
      ],
      "source": [
        "# Test BookCorpusParser on real data\n",
        "if book_file:\n",
        "    print(\"\\nProcessing BookCorpus sentences with BookCorpusParser...\\n\")\n",
        "    \n",
        "    book_parser = BookCorpusParser(cleaner)\n",
        "    output_file = temp_dir / \"bookcorpus_processed.txt\"\n",
        "    \n",
        "    count = book_parser.save_to_file(input_path=book_file, output_file=output_file)\n",
        "    \n",
        "    print(f\"\\nProcessed {count} passages from BookCorpus\")\n",
        "    print(f\"\\nFirst 3 processed passages:\")\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            if i > 3:\n",
        "                break\n",
        "            preview = line.strip()[:100] + \"...\" if len(line.strip()) > 100 else line.strip()\n",
        "            print(f\"  {i}. {preview}\")\n",
        "else:\n",
        "    print(\"Skipping BookCorpus processing (download failed)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
